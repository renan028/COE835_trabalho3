%---------------------------------------------------------------------
\section{Identificação de parâmetros}

Identificação de parâmetros é usar a coleção de sinais disponíveis do sistema,
baseado em algum critério de otimalidade,
para produzir uma estimativa dos parâmetros desconhecidos da planta.
Identificação adaptativa dos parâmetros é um procedimento de estimação dinâmica que faz uso da atualização
dos sinais do sistema para estimar os parâmetros desconehcidos, atualizados
on-line. A identificação adaptativa de parâmetros é crucial para o projeto de
controladores adaptativos, onde os parâmetros de controle devem ser atualizados 
on-line ao mesmo tempo em que o sistema está em operação.

Considere um sistema linear invariante no tempo descrito pela equação
diferencial (Eq. \ref{eq:planta}):
%
\begin{equation}
P(s) \, [y](t) = Z(s) \, [u](t),
\label{eq:planta}
\end{equation}

onde $y(t) \in \mathbb{R}$ e $u(t) \in \mathbb{R}$ são os sinais medidos de
saída e entrada do sistema, sendo:
%
\begin{gather}
P(s) = s^n + p_{n-1}s^{n-1}+\ldots + p_1s+p_0\,\\
Z(s) = z_ms^m + z_{m-1}s^{m-1}+\ldots + z_1s+z_0 \,,
\label{eq:poly}
\end{gather}
%
os polinômios em $s$, com $s$ sendo o operador de diferencial
$s[x](t)=\dot{x}(t)$. $p_i$, $i = 0,1,\ldots,n-1$ e $z_i$, $i=0,1,\ldots,m$ ($n>m$) são os parâmetros desconhecidos da planta.

Escolhe-se então um polinômio estável $\Lambda(s) =
s^n+\lambda_{n-1}s^{n-1}+\ldots+\lambda_1s+\lambda_0$ para filtrar a entrada e a saída da planta com o filtro $\frac{1}{\Lambda(s)}$. Multiplicando ambos os
lados da equação \ref{eq:planta} pelo filtro, temos:
%
\begin{equation}
y(t) = \frac{Z(s)}{\Lambda(s)} \, [u](t)+\frac{\Lambda(s)-P(s)}{\Lambda(s)} \, [y](t) \,.
\label{eq:filter_plant}
\end{equation}

Introduzindo o vetor de parâmetros reais $\theta^*$ e o regressor $\phi(t)$:
%
\begin{equation}
\theta^* =
  \begin{bmatrix}
  z_0 & z_1 & \ldots & z_m &
  (\lambda_0-p_0) & (\lambda_1-p_1) & \ldots & (\lambda_{n-1}-p_{n-1})
  \end{bmatrix}
^\intercal \in \mathbb{R}^{n+m+1} \,, 
\label{eq:parameters}
\end{equation}
%
\begin{equation}
\phi(t) =
  \begin{bmatrix}
    \frac{1}{\Lambda(s)}[u](t) & \frac{s}{\Lambda(s)}[u](t) & \ldots & \frac{s^{m}}{\Lambda(s)}[u](t) & \frac{1}{\Lambda(s)}[y](t) & \frac{s}{\Lambda(s)}[y](t) & \ldots & \frac{s^{n-1}}{\Lambda(s)}[y](t)
  \end{bmatrix}
^\intercal \in \mathbb{R}^{n+m+1} \,, 
\label{eq:regressor}
\end{equation}
%
podemos expressar \ref{eq:filter_plant} como:
%
\begin{equation}
y(t) = \theta^{*\intercal} \, \phi(t).
\label{eq:thetaphi}
\end{equation}

A implementação do filtro é feita pela construção de 2 sistemas
dinâmicos, na realização de estados:
%
\begin{gather}
\dot{\omega}_1(t) = A_\lambda \, \omega_1(t) + bu(t)\,,\\
\dot{\omega}_2(t) = A_\lambda \, \omega_2(t) + by(t)\,,
\label{eq:statespace}
\end{gather}
%
onde $\omega_1(t) \in \mathbb{R}^n$, $\omega_2(t) \in \mathbb{R}^n$ e:
%
\begin{equation}
A_\lambda = 
\begin{bmatrix}
    0      & 1      & 0      & \dots  & 0      & 0      \\
    0      & 0      & 1      & 0      & \dots  & 0      \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    0      & 0      & \dots  & \dots  & 0      & 1      \\
    -\lambda_0 & -\lambda_1 & \dots & \dots & -\lambda_{n-2} & -\lambda_{n-1} 
\end{bmatrix}
\in \mathbb{R}^{n \times n} \,, \quad b =
\begin{bmatrix}
    0  \\
    \vdots  \\
    0 \\
    1 \\ 
\end{bmatrix}
\in \mathbb{R}^n \,.
\label{eq:statespace}
\end{equation}

O vetor regressor $\phi(t)$ pode ser escrito como:
%
\begin{gather}
  \phi(t) =
  \begin{bmatrix}
    (C_m \, \omega_1(t))^\intercal & \omega_2^\intercal(t)
  \end{bmatrix}
  ^\intercal\,, \\
  C_m = 
  \begin{bmatrix}
    I_{m+1} & 0_{(m+1)\times(n-m-1)}
  \end{bmatrix}
  \,,
\end{gather} 
%
onde $I_{m+1}$ é a matriz identidade de dimensão $(m+1)$.
 
Considere $\theta(t)$ a estimativa dos parâmetros reais $\theta^*$. O erro de
estimação pode ser definido como:
%
\begin{equation}
\epsilon(t) = \theta^\intercal(t) \, \phi(t) - y(t) =
\tilde{\theta}^\intercal(t) \, \phi(t) \,, \enskip t \geq t_0 \,.
\end{equation}

Neste trabalho, serão considerados dois algoritmos para a atualização
da estimação dos parâmetros $\theta$: o método do \textbf{gradiente normalizado} e o método \textbf{\textit{least-square} normalizado}.

O algoritmo do \textit{gradiente normalizado} para atualização da estimação dos
parâmetros corresponde a escolher a derivada de $\theta(t)$ na direção do
gradiente descendente, minimizando a função custo normalizada:
%
\begin{gather}
J(\theta) = \frac{\epsilon^2}{2 \, m^2} =
\frac{\tilde{\theta}^\intercal \, \phi \, \phi^\intercal \, \tilde{\theta}}{2 \, m^2},\\[5pt]
m^2 = 1 + \kappa \, \phi^\intercal(t) \, \phi(t) \,, \enskip \kappa > 0 \,.
\end{gather}

Derivando-se a função custo e igualando a zero, obtém-se a lei de adaptação paramétrica:
%
\begin{gather}
\dot{\theta}(t) = -\frac{\Gamma \, \phi(t) \, \epsilon(t)}{m^2(t)} \,, \enskip \theta(t_0) = \theta_0 \,, \enskip t \geq t_0 \,,
\end{gather}
%
onde $\Gamma = \Gamma^\intercal > 0$ é uma matrix de ganhos positiva definida.

No caso do algoritmo \textit{least-square normalizado}, a atualização
paramétrica é dada pelas equações:
%
\begin{gather}
\dot{\theta}(t) = -\frac{P(t) \, \phi(t) \, \epsilon(t)}{m^2(t)} \,, \enskip \theta(t_0) = \theta_0 \,, \enskip t \geq t_0 \,,\\[5pt]
%
\dot{P}(t) = -\frac{P(t) \, \phi(t) \, \phi^\intercal(t) \, P(t)}{m^2(t)} \,, \enskip P(t_0) = P_0 = P_0^\intercal>0 \,, \enskip t \geq t_0 \,,\\[5pt]
%
m^2(t) = 1 + \kappa \, \phi^\intercal(t) \, P(t) \, \phi(t) \,, \enskip \kappa > 0 \,.
\end{gather}

Em ambos os algoritmos, vale observar que é possível provar por Lyapunov a estabilidade e convergência de $\epsilon = \theta^\intercal(t) \, \phi(t) - y(t)$ para zero. Porém, esta condição não garante $\tilde{\theta} \rightarrow 0$, mas apenas a ortogonalidade entre os vetores de erro dos parâmetros $\tilde{\theta}$ e o regressor $\phi$. A convergência dos parâmetros é garantida quando há excitação persistente do sistema através da escolha adequada do sinal de referência.			
