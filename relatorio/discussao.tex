%---------------------------------------------------------------------
\section{Discussão}

A simulação \#1 mostra o comportamento do sistema para variações no sinal de
referência. Como esperado e demonstrado na seção 3.5 do livro \textit{Adaptive
Control Design and Analysis} de Gang Tao, a convergência dos parâmetros só é
garantida quando o sinal de referência possui o mesmo número de
frequências que o número de parâmetros desconhecido (excitações persistentes).

Na simulação 1.1, o sistema é de primeira ordem com dois
parâmetros desconhecidos, logo são necessárias duas frequências diferentes no
sinal de referência para garantir que o erro de estimação $\tilde{\theta}$
convirja para zero. Isso é verificado, já que $\tilde{\theta}$ converge para
zero quando $r=1+5\textrm{sin}(t)$ (frequência em $\omega=0, \omega=1,
\omega=-1$) e não converge para zero quando o sinal possui apenas uma frequência
$r=1$. Comportamento semelhante é verificado para os sistemas de segunda e
terceira ordens, simulações 1.2 e 1.3, respectivamente. Vale observar que em
todos os casos $\epsilon = \tilde{\theta}^\intercal \phi$ converge para zero,
como é provado por Lyapunov.  Isso significa que, apesar de a estimação dos
parâmetros não convergir para zero, $\tilde{\theta}$ e $\phi$ entram em
quadratura (vetores ortogonais).

A simulação \#2 mostra o comportamennto do sistema para variações no
ganho de adaptação $\Gamma$. A variação da estimação de parâmetros é descrita
pela fórmula: $\dot{\theta}(t) = -\frac{\Gamma \, \phi(t) \,
\epsilon(t)}{m^2(t)}$. Portanto, a variação é proporcional ao ganho de
adaptação, quanto maior o parâmetro, mais rápida será a estimação.

A simulação \#3 mostra o comportamento do sistema para variações nas condições
iniciais. A rapidez da convergência depende de quão próximo o parâmetro estimado
está do parâmetros real. Na simulação 3.1, por exemplo, a convergência é mais
rápida quando $\theta = \textbf{0}, \theta^* = \left[1 \, -1\right]$, porém, na
simulação 3.3, ela é mais rápida quando $\theta = \textbf{1}, \theta^* = \left[1
\, 1 \, 1 \, 0 \, 2 \, 2\right]$.

Observe que o comportamento dos sistemas é semelhante para ambos os métodos
utilizados, \textit{Gradiente normalizado} e \textit{least-square
normalizado}. Pode ser provado que o método \textit{Gradiente
normalizado} garante convergência exponencial para zero (Tao 3.5.2), já o método
\textit{least-square normalizado} apenas garante convergência, mas não
exponencial (Tao 3.5.3).
