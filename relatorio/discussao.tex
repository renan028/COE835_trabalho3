%---------------------------------------------------------------------
\section{Discussão}

A \textbf{simulação \#1} mostra o comportamento do sistema para variações no sinal de referência. Como esperado e demonstrado na seção 3.5 do livro \textit{Adaptive
Control Design and Analysis} de Gang Tao, a convergência dos parâmetros só é
garantida quando o sinal de referência possui o mesmo número de
frequências que o número de parâmetros desconhecidos (excitações persistentes).

Na simulação 1.1, o sistema é de primeira ordem com dois
parâmetros desconhecidos. Logo, são necessárias duas frequências diferentes no
sinal de referência para garantir que o erro de estimação $\tilde{\theta}$
convirja para zero. Isso é verificado, já que $\tilde{\theta}$ converge para
zero quando $r=1+5\textrm{sin}(t)$ (frequência em $\omega=0, \omega=1,
\omega=-1$) e não converge para zero quando o sinal possui apenas uma frequência
($r=1$). Um comportamento semelhante é verificado para os sistemas de segunda e
terceira ordens (simulações 1.2 e 1.3, respectivamente). Vale observar que, em
todos os casos, $\epsilon = \tilde{\theta}^\intercal \phi$ converge para zero,
como é provado por Lyapunov.  Isso significa que, apesar de a estimação dos
parâmetros não ter garantia de convergência para zero, os vetores $\tilde{\theta}$ e $\phi$ entram em quadratura, ou seja, ficam ortogonais.

A \textbf{simulação \#2} mostra o comportamento do sistema para variações no
ganho de adaptação $\Gamma$ para o método do gradiente normalizado. Nesse caso, a variação da estimação de parâmetros é descrita pela fórmula: $\dot{\theta}(t) = -\Gamma \, \phi(t) \, \epsilon(t) / m^2(t)$. Portanto, a variação é proporcional ao ganho de adaptação, quanto maior o parâmetro, mais rápida será a estimação. Já no caso do método \textit{least-squares} normalizado, o valor inicial do ganho não impacta muito no regime transitório.

A \textbf{simulação \#3} mostra o comportamento do sistema para variações nas condições iniciais. A rapidez da convergência depende de quão próximo os parâmetros estimados estão dos parâmetros reais. Na simulação 3.1, por exemplo, a convergência é mais rápida quando $\theta = \textbf{0}, \theta^* = \left[1 \, -1\right]$, porém, na simulação 3.3, ela é mais rápida quando $\theta = \textbf{1}$, pois $\theta^* = \left[1 \, 1 \, 1 \, 0 \, 2 \, 2\right]$.

Observe que o comportamento dos sistemas é semelhante para ambos os métodos
utilizados: \textit{Gradiente normalizado} e \textit{least-square
normalizado}. Pode ser provado que o método \textit{Gradiente
normalizado} garante convergência exponencial do erro para zero (Tao 3.5.2). Já o método \textit{least-square normalizado} apenas garante convergência, mas não
exponencial (Tao 3.5.3).

Também foi constatada, mas não relatada aqui, a interessante propriedade do método \textit{least-square normalizado} em que $P^{-1} \, \tilde{\theta} = P_0^{-1} \, \tilde{\theta}(0) = \text{constante} \,$.